{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>task</th>\n",
       "      <th>verb</th>\n",
       "      <th>sentence</th>\n",
       "      <th>neg_sentence</th>\n",
       "      <th>complement</th>\n",
       "      <th>turker_pos_ratings</th>\n",
       "      <th>turker_neg_ratings</th>\n",
       "      <th>bert_pos_entailment_prob</th>\n",
       "      <th>bert_pos_contradiction_prob</th>\n",
       "      <th>bert_pos_neutral_prob</th>\n",
       "      <th>bert_neg_entailment_prob</th>\n",
       "      <th>bert_neg_contradiction_prob</th>\n",
       "      <th>bert_neg_neutral_prob</th>\n",
       "      <th>signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>to</td>\n",
       "      <td>move</td>\n",
       "      <td>Fearing that overdevelopment, as well as new t...</td>\n",
       "      <td>Fearing that overdevelopment, as well as new t...</td>\n",
       "      <td>Fearing that overdevelopment, as well as new t...</td>\n",
       "      <td>1.0,2.0,2.0</td>\n",
       "      <td>-2.0,-2.0,-2.0</td>\n",
       "      <td>0.998875</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.999224</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>o/o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>decline</td>\n",
       "      <td>Nike declined to be a sponsor.</td>\n",
       "      <td>Nike did not decline to be a sponsor.</td>\n",
       "      <td>Nike was a sponsor.</td>\n",
       "      <td>-2.0,-2.0,1.0</td>\n",
       "      <td>0.0,0.0,1.0</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.998916</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.996050</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>-/o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>to</td>\n",
       "      <td>intend</td>\n",
       "      <td>I intend to resume the habit as a delight of o...</td>\n",
       "      <td>I do not intend to resume the habit as a delig...</td>\n",
       "      <td>I resume the habit as a delight of old.</td>\n",
       "      <td>-2.0,1.0,1.0</td>\n",
       "      <td>0.0,0.0,0.0</td>\n",
       "      <td>0.997368</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.999205</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>o/o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>intend</td>\n",
       "      <td>Ashoka intended to use this power to control p...</td>\n",
       "      <td>Ashoka did not intend to use this power to con...</td>\n",
       "      <td>Ashoka used this power to control people.</td>\n",
       "      <td>0.0,0.0,1.0</td>\n",
       "      <td>-1.0,0.0,0.0</td>\n",
       "      <td>0.995333</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.998603</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>o/o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>intend</td>\n",
       "      <td>Yesterday I said that I intended to live by my...</td>\n",
       "      <td>Yesterday I said that I did not intend to live...</td>\n",
       "      <td>Yesterday i said that i lived by my wits.</td>\n",
       "      <td>0.0,1.0,2.0</td>\n",
       "      <td>-2.0,-2.0,0.0</td>\n",
       "      <td>0.997938</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.999453</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>o/o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index task     verb                                           sentence  \\\n",
       "0      0   to     move  Fearing that overdevelopment, as well as new t...   \n",
       "1      1   to  decline                     Nike declined to be a sponsor.   \n",
       "2      2   to   intend  I intend to resume the habit as a delight of o...   \n",
       "3      3   to   intend  Ashoka intended to use this power to control p...   \n",
       "4      4   to   intend  Yesterday I said that I intended to live by my...   \n",
       "\n",
       "                                        neg_sentence  \\\n",
       "0  Fearing that overdevelopment, as well as new t...   \n",
       "1              Nike did not decline to be a sponsor.   \n",
       "2  I do not intend to resume the habit as a delig...   \n",
       "3  Ashoka did not intend to use this power to con...   \n",
       "4  Yesterday I said that I did not intend to live...   \n",
       "\n",
       "                                          complement turker_pos_ratings  \\\n",
       "0  Fearing that overdevelopment, as well as new t...        1.0,2.0,2.0   \n",
       "1                                Nike was a sponsor.      -2.0,-2.0,1.0   \n",
       "2            I resume the habit as a delight of old.       -2.0,1.0,1.0   \n",
       "3          Ashoka used this power to control people.        0.0,0.0,1.0   \n",
       "4          Yesterday i said that i lived by my wits.        0.0,1.0,2.0   \n",
       "\n",
       "  turker_neg_ratings  bert_pos_entailment_prob  bert_pos_contradiction_prob  \\\n",
       "0     -2.0,-2.0,-2.0                  0.998875                     0.000317   \n",
       "1        0.0,0.0,1.0                  0.000543                     0.998916   \n",
       "2        0.0,0.0,0.0                  0.997368                     0.000648   \n",
       "3       -1.0,0.0,0.0                  0.995333                     0.001741   \n",
       "4      -2.0,-2.0,0.0                  0.997938                     0.000746   \n",
       "\n",
       "   bert_pos_neutral_prob  bert_neg_entailment_prob  \\\n",
       "0               0.000807                  0.000299   \n",
       "1               0.000542                  0.996050   \n",
       "2               0.001984                  0.000256   \n",
       "3               0.002926                  0.000586   \n",
       "4               0.001316                  0.000226   \n",
       "\n",
       "   bert_neg_contradiction_prob  bert_neg_neutral_prob signature  \n",
       "0                     0.999224               0.000477       o/o  \n",
       "1                     0.000405               0.003545       -/o  \n",
       "2                     0.999205               0.000540       o/o  \n",
       "3                     0.998603               0.000811       o/o  \n",
       "4                     0.999453               0.000322       o/o  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_table(\"verb_veridicality.tsv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = dataset['verb']\n",
    "signs = dataset['signature']\n",
    "datas = [(verbs[i], signs[i]) for i in range(len(verbs))]\n",
    "datas = set(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(datas), columns =['Verb', 'Signature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"verb_veridicality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"../data/ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thousands',\n",
       " 'of',\n",
       " 'demonstrators',\n",
       " 'have',\n",
       " 'marched',\n",
       " 'through',\n",
       " 'London',\n",
       " 'to',\n",
       " 'protest',\n",
       " 'the',\n",
       " 'war',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'and',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'withdrawal',\n",
       " 'of',\n",
       " 'British',\n",
       " 'troops',\n",
       " 'from',\n",
       " 'that',\n",
       " 'country',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = [[s[2] for s in sentence] for sentence in getter.sentences]\n",
    "print(labels[0])\n",
    "tag_values = list(set(data[\"Tag\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig, RobertaTokenizer\n",
    "from transformers import RobertaForTokenClassification, AdamW\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "MAX_LEN = 75\n",
    "bs = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(sentences, labels)\n",
    "]\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)-\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.3053, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1480, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3043, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2080, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2477, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2065, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1954, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1973, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2381, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2741, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1489, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1420, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1386, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1662, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2389, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2173, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1443, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1829, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1466, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1159, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1924, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1867, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1778, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1570, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1629, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2807, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1590, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1260, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1512, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2169, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2178, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1822, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1613, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2396, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1979, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1637, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1937, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1669, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1401, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1197, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1685, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1983, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1052, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2120, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1687, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1470, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2131, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1188, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1827, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2433, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2125, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0783, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2342, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1474, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1176, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1903, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1254, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2753, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1367, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1814, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2272, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1682, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1397, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1349, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1450, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1387, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1845, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1358, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1537, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1429, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1712, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0777, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1754, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1476, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1085, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1440, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0887, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1447, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1715, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1575, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1281, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1383, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1030, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2521, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1400, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1267, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1811, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2988, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1345, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1295, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1794, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1561, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1215, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1881, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1940, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1959, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2348, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1819, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1457, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0956, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1244, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1714, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1214, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1725, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1045, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1492, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1171, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2395, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1308, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2057, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2109, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1889, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1328, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1747, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1406, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2571, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1931, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1412, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2083, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0801, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1432, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1333, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1529, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1773, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1997, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1217, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0891, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2050, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1104, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1462, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1494, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1879, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1932, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2323, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1442, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1757, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1240, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1851, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1842, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1023, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1126, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2177, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1856, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1792, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1611, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1096, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1405, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2277, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1298, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1302, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0908, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1711, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1675, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1488, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2115, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1941, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1586, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0790, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0963, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1338, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2808, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1713, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1415, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1736, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1172, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2263, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2352, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1884, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1134, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1651, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1249, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1986, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0999, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1234, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1839, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1528, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1639, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1694, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1872, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1961, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1960, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0895, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1343, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1451, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1957, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2010, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1209, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2024, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1519, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0607, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1497, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1750, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1513, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1162, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1268, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1539, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1628, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0952, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1313, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1740, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1526, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1483, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1820, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1463, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1892, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1324, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2316, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1431, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1666, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1569, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1285, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1574, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2022, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1907, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1165, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0912, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1719, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2105, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1559, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1672, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1257, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1837, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0452, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0955, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1532, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0935, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1911, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2239, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1579, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1885, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1143, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1437, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2414, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1091, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0848, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1728, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0831, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0930, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1229, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1635, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1552, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1269, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1768, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1832, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2242, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1873, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1572, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1163, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1558, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1709, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0503, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0840, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0733, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1329, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1600, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1929, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1984, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1555, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0647, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0860, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2251, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1376, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1516, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1009, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1825, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1878, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1481, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1264, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1187, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1428, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1578, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2300, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1738, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2517, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1502, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0916, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1086, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0576, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1422, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1698, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1278, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1657, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1107, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2676, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1127, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1317, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1365, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1943, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1067, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1094, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1179, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1795, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1733, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1305, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1550, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1032, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0965, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1210, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1404, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1048, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1796, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1410, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2079, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1340, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1241, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1416, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1226, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1486, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2144, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1034, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1310, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1852, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1236, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1319, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1802, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1612, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1649, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1479, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1621, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1330, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1739, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0898, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1780, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1129, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1527, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1391, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1530, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1223, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1970, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1577, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1082, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1619, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0869, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1645, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1292, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2200, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2075, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2066, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1439, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1332, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1182, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1511, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1818, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2029, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1465, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1609, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1592, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1745, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2224, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0596, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2390, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1910, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1327, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1312, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1435, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1580, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1290, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2028, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1756, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1121, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1874, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1644, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1175, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2222, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0855, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1493, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2684, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1013, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0761, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1683, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1363, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1344, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1546, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1315, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1202, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1293, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0763, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1183, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1193, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1593, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2252, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1554, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1007, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1253, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1361, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1587, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1286, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1084, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0972, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1925, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1659, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0861, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1296, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1849, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1326, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1228, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2154, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1991, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1364, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1378, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1309, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1088, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1136, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1114, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1354, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1573, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1148, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1371, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1844, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1166, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1487, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1646, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0901, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1273, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1245, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1398, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2653, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1355, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1540, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1303, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0737, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1458, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1012, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2071, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1865, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.3265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1626, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1427, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1982, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1346, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1380, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1160, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1314, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1562, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1161, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1655, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0817, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1018, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1385, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0824, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1632, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0604, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1142, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0793, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1037, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1971, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2233, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1219, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1882, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1509, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1475, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0707, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1388, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1441, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1158, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1640, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1280, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1339, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1448, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2098, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0843, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1543, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1133, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1266, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1484, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1921, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0904, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1077, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1507, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1816, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0727, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1421, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1606, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1419, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1227, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1510, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1525, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2035, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1392, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2014, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1638, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1482, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1097, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1092, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0799, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1731, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1436, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1186, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0764, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1472, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1496, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1938, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2449, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1464, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1138, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1181, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1199, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1896, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1331, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0918, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Loss:  tensor(0.1135, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1124, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1054, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1243, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1453, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1866, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1174, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1547, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1656, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2168, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1325, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1868, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1350, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2157, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2695, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1375, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1446, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1283, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1614, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1020, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1434, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1382, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1498, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2259, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2291, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1409, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1384, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1137, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2064, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1544, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1418, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1195, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1473, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1643, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0923, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1603, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1581, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1776, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1699, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1335, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2056, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1248, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0992, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0927, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1706, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1926, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1297, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1642, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1717, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1351, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1411, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1042, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1667, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1703, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2060, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1627, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1499, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1455, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0998, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1977, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2192, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1111, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1582, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1518, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1191, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.2058, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0877, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0791, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1311, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.0854, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1044, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Local Loss:  tensor(0.1362, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Average train loss: 0.1576315756304517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [19:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.13975810597340266\n",
      "Validation Accuracy: 0.9567322861650319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Found input variables without list of list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-32d3329a389e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                   for l_i in l if tag_values[l_i] != \"PAD\"]\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation F1-Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, average, suffix, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                                                      \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                                                      suffix=suffix)\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'average has to be one of {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mpred_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mis_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found input variables without list of list.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen_true\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Found input variables without list of list."
     ]
    }
   ],
   "source": [
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        print(\"Local Loss: \", loss)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Mr. Trumps tweets began just moments after a Fox News report by Mike Tobin, a\" + \\\n",
    "\"reporter for the network, about protests in Minnesota and elsewhere. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t<s>\n",
      "B-per\tMr\n",
      "B-per\t.\n",
      "I-per\tTrump\n",
      "O\t\n",
      "O\t\n",
      "O\ts\n",
      "O\ttweets\n",
      "O\tbegan\n",
      "O\tjust\n",
      "O\tmoments\n",
      "O\tafter\n",
      "O\ta\n",
      "B-org\tFox\n",
      "B-org\tNews\n",
      "O\treport\n",
      "O\tby\n",
      "B-per\tMike\n",
      "I-per\tTob\n",
      "I-per\tin\n",
      "O\t,\n",
      "O\tare\n",
      "O\tporter\n",
      "O\tfor\n",
      "O\tthe\n",
      "O\tnetwork\n",
      "O\t,\n",
      "O\tabout\n",
      "O\tprotests\n",
      "O\tin\n",
      "B-geo\tMinnesota\n",
      "O\tand\n",
      "O\telsewhere\n",
      "O\t.\n",
      "O\t\n",
      "O\t</s>\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
